---
title: "FinalExam-Nev-Simmons.R"
author: "Simmons, Nev"
date: "2024-12-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) #Eliminates errors and long messages
```

```{r}
library(tidyverse)  #Load in Packages
library(caret)
library(knitr)
```

#Part 1

```{r}
diabetesdata <- read_csv("diabetes_data.csv")  #Read Data

diabetesdata <- diabetesdata %>%          #Change data into Factors!
  mutate(diabetes = as.factor(diabetes),   #Binary Response!
         gender = as.factor(gender),
         hypertension = as.factor(hypertension),
         heart_disease = as.factor(heart_disease),
         smoking_history = as.factor(smoking_history))
```

```{r}
set.seed(1211)
DiabetesIndex <- createDataPartition(1:nrow(diabetesdata), p=0.80)$Resample1   #Partition Data

diabetes_train <- filter(diabetesdata, row_number() %in% DiabetesIndex)   #Create Subset of Data
diabetes_test <- filter(diabetesdata, !row_number() %in% DiabetesIndex)

train_control <- trainControl(method = "repeatedcv", number = 5, repeats=10)   #Make CV with 10 repeats
```

```{r, cache=TRUE}
set.seed(1211)
diabetes.log <- train(diabetes ~ gender + age + hypertension + heart_disease + 
                       smoking_history + bmi + HbA1c_level + blood_glucose_level,
                data=diabetes_train, 
                method="glm",
                family = "binomial",
                trControl=train_control)
logvarimp <- varImp(diabetes.log)  #Make first logistic regression
logvarimp
```


```{r singleTree, cache=TRUE}
set.seed(1211)
diabetes.tree <- train(diabetes ~ HbA1c_level + age + blood_glucose_level + bmi,
                data=diabetes_train,
                method="rpart",
                trControl=train_control,
                tuneLength=5)
treevarimp <- varImp(diabetes.tree)  #Make Regression Tree
treevarimp
```

```{r randomForest, cache=TRUE}
set.seed(1211)
diabetes.rf <- train(diabetes ~ gender + age + hypertension + heart_disease + 
                       smoking_history + bmi + HbA1c_level + blood_glucose_level, 
                data=diabetes_train,
                method="rf",
                trControl=train_control,
                tuneGrid=expand.grid(mtry=1:4),
                ntree = 500,
                importance=TRUE)
variableimp <- varImp(diabetes.rf$finalModel)   #Make Random Forest
variableimp
```

Based on the variable importance of the random forest model, the HbA1c level seems to be the 
most important predictor of diabetes with a score of 91.94. Blood glucose level seems to be 
the next highest predictor of diabetes with a score of 58.7. The next three highest variables 
are age with a score of 41.06, positive for hypertension or high blood pressure with 18.51 and 
lastly positive for heart disease with 13.56. As far as the models are concerned, I chose a 
logistic regression to get a relatively simple model to predict every variable. This gave me 
a good baseline as it is the least complex model chosen. Next I chose a single regression tree 
to get a little more complex but this time only test a small subset of variables. For the variables 
chosen I picked variables I would think would have the most importance in determining diabetes based 
on information learned in biology and prior knowledge of diabetes. Finally I took the most complex 
route of random forest to discover the variable importance as well as get a final baseline using the 
most complex prediction. I chose all predictors to understand the importance of every single predictor 
rather than just a small subset. 

# Part 2

```{r}
models.summary <- resamples(list(Logistic= diabetes.log,
                         SingleTree = diabetes.tree,
                         RandomForest= diabetes.rf))   #Find Accuracy Scores
summary(models.summary)
```
```{r}
diabetes_test %>%
  mutate(LogPred = predict(diabetes.log, newdata=diabetes_test),
         TreePred = predict(diabetes.tree, newdata=diabetes_test),
         RanForPred = predict(diabetes.rf, newdata=diabetes_test)) %>%
  summarize(LogAccuracy = mean(LogPred == diabetes),
            TreeAccuracy = mean(TreePred == diabetes),
            RanAccuracy = mean(RanForPred == diabetes))   #Test on Test Set
                
```



```{r}
BoxWhiskerPlot <- bwplot(models.summary, metric="Accuracy") #BW Plot comes from Dr. Kim's 363 In Class 20
# Also in Module 3 part 2 of this class
BoxWhiskerPlot
```
Since Diabetes was changed to a factored variable, we are no longer treating it 
with a continuous response variable. Now this variable is a factored variable
that denotes either 1 or 0 and nothing in between. Based on our Accuracy values which
shows the amount of agreement with the models predictions, we can find that in the
training set the random forest tree although overlapping, seems to be the best 
at predicting the diabetes response variable based on using all of the predictors available.
However, using the test set, the accuracy variable shows that the random forest was 
the worst performing method compared to the other two, and the regression tree was the
best at prediction. This is most likely due to the overfitting that occurs when
using every single predictor rather than using just four in the regression tree.
The problem with this comparison is that one of the models "tree", is using less predictors
and this shows to be just slightly better at prediction rather than the simple logistic 
regression model, most likely due to the less amount of predictors and the more
concise predictors chosen. There is also a lot of error when considering these models,
with the over-lappage of these plots it may be hard to completely trust these results. 
A precision test could be useful here as it would tell us how many false positives
are predicted by the results. Based on the value, we could determine if the true
prediction test is worth using to predict Diabetes, as false positives are a big
no-no in the medical field.


```{r}
HBA1cViolinPlot <- ggplot(diabetesdata) +
  geom_violin(aes(x = diabetes, y = HbA1c_level, fill = diabetes, alpha = 0.4)) +
  geom_boxplot(aes(x = diabetes, y = HbA1c_level), width = 0.1, outlier.shape = NA) +
  stat_summary(aes(x = diabetes, y = HbA1c_level), fun = mean, shape = 20, size = 1, color = 'blue') +
  scale_y_continuous(limits = c(3, 9)) +
   labs(title = "HbA1c Level as a Predictor For Diabetes",
        caption = "Based on this Violin plot, a good pick for a binary response variable, \
       it appears that the positive test of diabetes does have a large amount of correlation\
       with this predictor of HbA1c. Of course this would make sense considering this statistic \
       is judging the highness of your blood sugar over a period of time.",
       y = "HbA1C Level", 
       x = "Test Result for Diabetes") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 13.5),
        axis.title = element_text(size = 11),
        plot.caption = element_text(hjust = 0.5, size = 11),
        panel.grid.major.x = element_blank(),   #Get rid of horizontal lines
        panel.grid.minor = element_blank()) 
HBA1cViolinPlot
```

```{r}
HBA1cDensityPlot <- ggplot(diabetesdata, aes(x = HbA1c_level, fill = factor(diabetes))) +
  geom_density(alpha = 0.4) +
  labs(title = "HbA1c Level as a Predictor For Diabetes",
       subtitle = "",
       y = "Density", 
       x = "HbA1c Level",
       caption = "This density plot appears to show less of a drastic difference as \
       shown by the violin plot, but still a noticeable right skewed dataset towards the \
       larger values of HbA1c correlating with a positive test result. However, the large portion \
       of the data is largely overlapped so this is not a perfect indicator.") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 13.5),
        axis.title = element_text(size = 11),
        plot.caption = element_text(hjust = 0.5, size = 11),
        panel.grid.major.x = element_blank(),   #Get rid of horizontal lines
        panel.grid.minor = element_blank()) 
HBA1cDensityPlot
```


```{r}
AgeDensityPlot <- ggplot(diabetesdata, aes(x = age, fill = factor(diabetes))) +
  geom_density(alpha = 0.4) +
  labs(title = "Age as a Predictor For Diabetes",
       subtitle = "Y-Scale in distribution of occurences",
       caption = "Although age was not the 1st or 2nd highest of variable importance, \
       it appears to play a large role when considering the aging effect on the diabetes risk. \
       Specifically after around the age of 45 there is a sharp spike in density of positive results.",
       y = "Density", 
       x = "Age in Years") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 13.5),
        axis.title = element_text(size = 12),
        plot.caption = element_text(hjust = 0.5, size = 11),
        panel.grid.major.x = element_blank(),   #Get rid of horizontal lines
        panel.grid.minor = element_blank())
AgeDensityPlot
```

```{r}
BGLboxplot <- ggplot(diabetesdata) +
  geom_boxplot(aes(x= blood_glucose_level, y = diabetes)) + 
  labs(title = "Blood Glucose Level as a Predictor For Diabetes",
       x = "Blood Glucose Level", 
       y = "Test Result for Diabetes",
       caption = "To fully grasp the results, a boxplot was chosen to show the IQR in a \
       positive result skewing the data quite considerably. The means in this predictor are \
       quite similar, but the shear range of positive diabetes cases with a high blood sugar\
       level make this data quite skewed. This does not appear to be the best predictor of Diabetes.") +
  scale_y_discrete(breaks = c("0", "1"),
                   labels = c("Negative", "Positive")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
        axis.title = element_text(size = 11),
        plot.caption = element_text(hjust = 0.5, size = 11),
        panel.grid.major.x = element_blank(),   #Get rid of horizontal lines
        panel.grid.minor = element_blank()) 
BGLboxplot
```


```{r, fig.width=17, fig.height=12}
library(patchwork)
Dashboard <- (HBA1cViolinPlot | HBA1cDensityPlot) / (AgeDensityPlot | BGLboxplot)
Dashboard
```

```{r}
ggsave("FinalExam-Dashboard.png", Dashboard, width = 17, height = 12, dpi = 300)
png("BoxWhiskerPlot.png", width = 800, height = 600)
print(BoxWhiskerPlot)
```
